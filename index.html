<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>DART</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://dorverbin.github.io/refnerf">
    <meta property="og:title" content="Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields">
    <meta property="og:description" content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields">
    <meta name="twitter:description" content="Neural Radiance Fields (NeRF) is a popular view synthesis technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location. While NeRF-based techniques excel at representing fine geometric structures with smoothly varying view-dependent appearance, they often fail to accurately capture and reproduce the appearance of glossy surfaces. We address this limitation by introducing Ref-NeRF, which replaces NeRF's parameterization of view-dependent outgoing radiance with a representation of reflected radiance and structures this function using a collection of spatially-varying scene properties. We show that together with a regularizer on normal vectors, our model significantly improves the realism and accuracy of specular reflections. Furthermore, we show that our model's internal representation of outgoing radiance is interpretable and useful for scene editing.">
    <meta name="twitter:image" content="https://dorverbin.github.io/refnerf/img/refnerf_titlecard.jpg">


    <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%E2%9C%A8&lt;/text&gt;&lt;/svg&gt;">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <b>DART</b>: Articulated Hand Model with <br> Diverse Accessories and Rich Textures<br>
                <small>
                     Neurips 2022 dataset & benchmark track (under review)
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://github.com/tomguluson92">Daiheng Gao
                            </a>
                            <br>Alibaba XR Lab<br>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://xiuyuliang.cn/">Yuliang Xiu
                            </a>
                            <br>Max Planck Institute for Intelligent Systems<br>
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://kailinli.top/#">
                             Kailin Li
                            </a>
                            <br>SJTU MVIG
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <a style="text-decoration:none" href="https://lixiny.github.io/">
                                Lixin Yang
                            </a>
                            <br>SJTU MVIG
                        </td>
                        <td>
                            <a style="text-decoration:none">
                              Feng Wang
                            </a>
                            <br>Alibaba XR Lab
                        </td>
                        <td>
                            <a style="text-decoration:none">
                              Peng Zhang
                            </a>
                            <br>Alibaba XR Lab
                        </td>
                    </tr>
                    <tr>
                        <td>
                            <a style="text-decoration:none">
                              Bang Zhang
                            </a>
                            <br>Alibaba XR Lab
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.mvig.org/">
                              Cewu Lu
                            </a>
                            <br>SJTU MVIG
                        </td>
                        <td>
                            <a style="text-decoration:none" href="https://www.cs.sfu.ca/~pingtan/">
                              Ping Tan
                            </a>
                            <br>Simon Fraser University
                        </td>
                    </tr>
                </table>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://openreview.net/forum?id=FPgCB_Z_0O">
                            <img src="./img/paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtube.com/embed/VvlUYe-9b7U">
                            <img src="./img/youtube_icon.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
<!--                        <li>-->
<!--                            <a href="https://storage.googleapis.com/gresearch/refraw360/ref_real.zip" target="_blank">-->
<!--                            <image src="img/real_database_icon.png" height="60px">-->
<!--                                <h4><strong>Real Dataset</strong></h4>-->
<!--                            </a>-->
<!--                        </li>                            -->
                        <li>
                            <a href="https://drive.google.com/file/d/1iqymPPPSF_rlKbHRvvgaVHlcmPsoEx25/view?usp=sharing" target="_blank">
                            <image src="img/google-drive.png" height="60px">
                                <h4><strong>DART GUI & Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <div class="video-compare-container" id="materialsDiv">-->
<!--                    <video class="video" id="materials" loop playsinline autoPlay muted src="video/materials_circle_mipnerf_ours.mp4" onplay="resizeAndPlay(this)"></video>-->
<!--                    -->
<!--                    <canvas height=0 class="videoMerge" id="materialsMerge"></canvas>-->
<!--                </div>-->
<!--			</div>-->
<!--        </div>-->
        <image src="img/teaser1.jpg" class="img-responsive" alt="overview" width="60%" style="max-height: 450px;margin:auto;">

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <div class="text-justify">
                    Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of 3D digital avatars. Among different hand morphable models, MANO has been widely used in various vision & graphics tasks. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic & lifestyle hand data. In this paper, we extend MANO with more Diverse Accessories and Rich Textures, namely <b>DART</b>. <b>DART</b> is comprised of 325 exquisite hand-crafted texture maps which varies in appearance, and covers different kinds of blemishes, make-ups and accessories. We also provide the Unity GUI which allows people to render hands with user-specific settings, e.g pose, camera, background, lighting, and <b>DART</b>'s textures. In this way, we generate large-scale (800K), diverse, and high-fidelity hand images, paired with perfect-aligned 3D labels, called <b>DARTset</b>. Experiments demonstrate its superiority in generalization and diversity. As a great complement for existing datasets, <b>DARTset</b> could boost hand pose estimation & surface reconstruction tasks. <b>DART</b> and Unity software is publicly available for research purpose.                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://youtube.com/embed/VvlUYe-9b7U" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>

<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Reflection Direction Parameterization-->
<!--                </h3>-->
<!--                <div class="text-justify">-->
<!--                    Previous approaches directly input the camera's view direction into the MLP to predict outgoing radiance. We show that instead using the reflection of the view direction about the normal makes the emittance function significantly easier to learn and interpolate, greatly improving our results.-->
<!--                    -->
<!--                    <br><br>-->
<!--                    -->
<!--                </div>-->
<!--                <div class="text-center">-->
<!--                    <video id="refdir" width="40%" playsinline autoplay loop muted>-->
<!--                        <source src="video/reflection_animation.mp4" type="video/mp4" />-->
<!--                    </video>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Comparision between DART and MANO basic topology.
                </h3>
                <div class="text-justify">
                    In DART, we remould the standard template hand mesh of MANO, which has 778 vertices and 1,538 faces, to a wrist-enhanced template mesh of
842 vertices and 1,666 faces. The pose parameter <img src="./img/dart_eq1.jpg" width="25%"> that drive the template hand mesh in MANO can be used as a direct placement for DART without any modifications.
                    <br><br>
                </div>
                <div class="text-center">
                    <img src="./img/dart_vs_mano.png" width="50%">
                </div>
			</div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    How to use DART tool in generating your own data?
                </h3>

                <h4>
                Step1: click <a href="https://drive.google.com/file/d/1iqymPPPSF_rlKbHRvvgaVHlcmPsoEx25/view?usp=sharing">DART GUI & Code</a> and download <font color="red">Build_Hand.zip</font>.
                    <div class="text-center">
                    <img src="./img/gui/step1.jpg" width="50%">
                </div>
                </h4>

                <h4>
                Step2: Pose Editing: allow arbitrarily, illumination, accessory, skin color and other terms.
                    <div class="text-center">
                    <img src="./img/gui/step2.jpg" width="50%">
                </div>
                    <td align="left" valign="top" width="50%">
                            <video id="v2" width="100%" playsinline autoplay loop muted>
                                <source src="video/step2.mp4" type="video/mp4" />
                            </video>
                        </td>
                </h4>

                <h4>
                    Step3: Exporting: rendered image with GT(mano pose, 2d/3d joint are put into the <font color="brown">output.pkl</font>)
                    <div class="text-center">
                    <img src="./img/gui/step3.jpg" width="50%">
                </div>
                    <td align="left" valign="top" width="50%">
                            <video id="v3" width="100%" playsinline autoplay loop muted>
                                <source src="video/step3.mp4" type="video/mp4" />
                            </video>
                        </td>
                </h4>
<!--                <div class="video-compare-container" style="width: 100%">-->
<!--                    <video class="video" id="toycar" loop playsinline autoPlay muted src="video/toycar.mp4" onplay="resizeAndPlay(this)"></video>-->
<!--                    <canvas height=0 class="videoMerge" id="toycarMerge"></canvas>-->
<!--                </div>-->
			</div>
        </div>


            <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    DARTset (Working in progress, please wait).
                </h3>

                <h4>
                    <div class="text-center"> Stand-alone document
                </div>
                </h4>
                <h4>
                    <div class="text-center"> DARTset link
                </div>
                </h4>
			</div>
        </div>

<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Scene Editing-->
<!--                </h3>-->
<!--                <div class="text-justify">-->
<!--                    We show that our structured representation of the directional MLP allows for scene editing after training. Here we show that we can convincingly change material properties.-->
<!--                    <br>-->
<!--                    We can increase and decrease material roughness:-->
<!--                </div>-->

<!--                <div style="overflow: hidden;">-->
<!--                    <video id="editing-materials" width="100%" playsinline autoplay loop muted style="margin-top: -5%;">-->
<!--                        <source src="video/materials_rougher_smoother.mp4" type="video/mp4" />-->
<!--                    </video>-->
<!--                </div>-->

<!--                <div class="text-justify">-->
<!--                    We can also control the amounts of specular and diffuse colors, or change the diffuse color without affecting the specular reflections:-->
<!--                </div>-->
<!--                -->
<!--                <table width="100%">-->
<!--                    <tr>-->
<!--                        <td align="left" valign="top" width="50%">-->
<!--                            <video id="v2" width="100%" playsinline autoplay loop muted>-->
<!--                                <source src="video/car_color2.mp4" type="video/mp4" />-->
<!--                            </video>-->
<!--                        </td>-->
<!--                        <td align="left" valign="top" width="50%">-->
<!--                            <video id="v3" width="100%" playsinline autoplay loop muted>-->
<!--                                <source src="video/car_color3.mp4" type="video/mp4" />-->
<!--                            </video>-->
<!--                        </td>-->
<!--                    </tr>-->
<!--                </table>-->

<!--            </div>-->
<!--        </div>-->

            
<!--        <div class="row">-->
<!--            <div class="col-md-8 col-md-offset-2">-->
<!--                <h3>-->
<!--                    Citation-->
<!--                </h3>-->
<!--                <div class="form-group col-md-10 col-md-offset-1">-->
<!--                    <textarea id="bibtex" class="form-control" readonly>-->
<!--@article{verbin2021refnerf,-->
<!--    title={{Ref-NeRF}: Structured View-Dependent Appearance for-->
<!--           Neural Radiance Fields},-->
<!--    author={Dor Verbin and Peter Hedman and Ben Mildenhall and-->
<!--            Todd Zickler and Jonathan T. Barron and Pratul P. Srinivasan},-->
<!--    journal={CVPR},-->
<!--    year={2022}-->
<!--}</textarea>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->

            <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    License
                </h3>
                <p class="text-justify">
                Codes are MIT license. GUI(Unity) tools are CC BY-NC 4.0 license. Dataset is CC BY-NC-ND 4.0 license.
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    <br>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>


</body></html>
